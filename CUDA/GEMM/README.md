# GEMM

- **HPC优化的核心思想：怎么样让数据放在更近的存储上来掩盖计算的延时，从而减少存储墙的影响**
- **单算子的优化思路**
    - **带宽**：尽可能提高单条指令的效率
    - **延迟**：通过流水尽可能掩盖指令间的延迟


## Global memory
- 在GPU中，一共开启$m\times n$个thread，每个thread需要读取矩阵A的一行与矩阵B的一列，而后将计算结果写回至矩阵C中
    - 即每个thread负责C矩阵中的一个元素的计算
- 一共需要从**global memory**中进行$2\times m\times n\times k$次读操作和$m\times n$次写操作

- 问题：主题循环由2条load和1条FMA组成，计算访存指令比1/3，导致了访存延迟不能被隐藏，从而性能不理想

- 分析：对于FP32
  - 延迟
    - 每一个乘法运算需要读2次内存和1次FFMA
- 假如没有其他额外的优化（如循环展开与指令重排），相当于是两个级联的自动扶梯，一个负责运送数据，一个负责做数学运算
- 一次 FFMA 需要等待 40s（访存）+ (1/0.5)s（第一个数据到达后第二个数据到达的时间）才能拿到所需的数据
    - vs Peak Performance 0.5s per FFMA
    - 这个 kernel 就完全被延迟卡住了，而无法发挥出应有的性能

### 带宽

- 计算访存比 = `64OP` / `132B` = **`0.48`**
    - 1个warp做32次FFMA，对应`64OP`
    - 需要读取读取 A 矩阵 1 个元素和 B 矩阵 32 个元素，共 `132B`
    - C通过寄存器累加，且忽略C矩阵开销
    - 虽然 dram 最小访问单位为一个 memory transaction，但考虑到 L1 cache 的存在也不会影响实际的计算访存比

shared memory

